{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1WvjkfSYy-Y7C6kqq9PIajBvRH28FqSTg",
      "authorship_tag": "ABX9TyPe5JLopDZnol6BmxYKZqw1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iasonkoutsoulis/Tralgo/blob/main/Tralgo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9198_m5mtMzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5333440e-a9bc-486c-c931-8694cdfc9a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# This is the algorithm I'll use to do automated trading.\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from datetime import datetime\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "\n",
        "def link_collect(soop):\n",
        "    all_linx = []\n",
        "    for link in soop.find_all('a'):\n",
        "        nlink = link.get('href')\n",
        "        all_linx.append(nlink)\n",
        "        all_linx = list(filter(lambda item: item is not None, all_linx))\n",
        "    return all_linx\n",
        "\n",
        "def year_collect(soop):\n",
        "    years = []\n",
        "    for timet in soop.find_all('time', {'class': 'fc-date-headline'}):\n",
        "        years.append(re.findall(r'\\d+', timet.string)[-1])\n",
        "    years = list(dict.fromkeys(years))\n",
        "    return years\n",
        "\n",
        "def tl_collect(all_links, years):\n",
        "    for yeart in years:\n",
        "        expr = r'https:\\/\\/www\\.theguardian\\.com\\/\\S+\\/' + yeart + r'\\/\\S+'\n",
        "        text_links = []\n",
        "        for link in all_links:\n",
        "            if re.search(r'/all$', link):\n",
        "                pass\n",
        "            elif re.search(expr, link):\n",
        "                text_links.append(link)\n",
        "    return text_links\n",
        "\n",
        "#\n",
        "# main script\n",
        "\n",
        "bimon_arts = dict()\n",
        "for page in range(287, 0, -1):\n",
        "    print(str(page))\n",
        "\n",
        "    #\n",
        "    # initialize using the front-page links\n",
        "\n",
        "    url = 'https://www.theguardian.com/business/stock-markets?page=' + str(page) # total pages = 287\n",
        "    html = requests.get(url).text\n",
        "    soup = bs(html, 'lxml')\n",
        "\n",
        "    #\n",
        "    # get all article links from the page we've opened (we use the year they include to identify them)\n",
        "\n",
        "    all_links = link_collect(soup)\n",
        "    years = year_collect(soup)\n",
        "    text_links = tl_collect(all_links, years)\n",
        "\n",
        "    #\n",
        "    # now we open all of the articles on the page and collect them into our bimonthly datasets\n",
        "    # we create a dictionary/log entry which holds all text for a span of 15 days.\n",
        "\n",
        "    for tlink in text_links:\n",
        "        subhtml = requests.get(tlink).text\n",
        "        subsoup = bs(subhtml, 'lxml')\n",
        "        texpr = r'^.*?(?= \\||$)'\n",
        "        try:\n",
        "          title = re.search(texpr, subsoup.title.string).group(0)\n",
        "        except Exception:\n",
        "          pass\n",
        "\n",
        "        timet = subsoup.find('meta', {'property':'article:published_time'})\n",
        "        try:\n",
        "            fdate = timet['content']\n",
        "        except Exception:\n",
        "            pass\n",
        "        dt_date = datetime.strptime(fdate, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "        art_date = str(dt_date.year) + '-' + str(dt_date.month)\n",
        "        bimon = 'B2' if dt_date.day >= 15 else 'B1'\n",
        "\n",
        "        if not (art_date + '-' + bimon) in bimon_arts:\n",
        "            bimon_arts[art_date + '-' + bimon] = []\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        article = [title]\n",
        "        for textlink in subsoup.find_all('p'):\n",
        "            article.append(textlink.string)\n",
        "            article = list(filter(lambda item: item is not None, article))\n",
        "            art_str = \" \".join(article)\n",
        "        if art_str in bimon_arts[art_date + '-' + bimon]:\n",
        "            pass\n",
        "        else:\n",
        "            bimon_arts[art_date + '-' + bimon].append(art_str)\n",
        "\n",
        "#\n",
        "# instead of text files I'll try the JSON stuff now\n",
        "\n",
        "with open('/content/drive/MyDrive/Tralgo articles/article_container.json', \"w\") as f:\n",
        "    json.dump(bimon_arts, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this fetches the financial data and tests for stationarity\n",
        "import numpy as np\n",
        "import pyarrow.feather as feather\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "import yfinance as yf\n",
        "\n",
        "tics = 'msft aapl goog tsla'\n",
        "\n",
        "df = yf.download(tics, interval = \"1wk\", start='2008-10-01')\n",
        "df = df[['Adj Close']].dropna()\n",
        "df.columns = df.columns.droplevel()\n",
        "df.index = df.index.strftime('%Y-%m') + '-' + np.where(df.index.day>=15, 'B2', 'B1')\n",
        "df = df.groupby(df.index).mean()\n",
        "for col in df:\n",
        "    df[col + '_dif'] = df[col].diff()\n",
        "    df[col + '_indicator'] = np.where(df[col + '_dif'] >=0, 1, 0)\n",
        "    df[col + '_future_indicator'] = df[col + '_indicator'].shift(-1)\n",
        "\n",
        "adfuller(df['GOOG_dif'].dropna())\n",
        "\n",
        "feather.write_feather(df, '/content/drive/MyDrive/Tralgo articles/financial_container.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1MSllihpQC9",
        "outputId": "350e3bca-d206-46a9-c63f-20882982f8b0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%***********************]  4 of 4 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "with the present we'll train the text data along with the financial data and get our trained net.\n",
        "this will require some manipulation of the current state of the data, which makes sense that is done here,\n",
        "in order to promote homogeneity and peace of mind...\n",
        "'''\n",
        "from datetime import datetime\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow.feather as feather\n",
        "import re\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "\n",
        "#\n",
        "# Fetch articles data\n",
        "\n",
        "with open('/content/drive/MyDrive/Tralgo articles/article_container.json', 'r') as f:\n",
        "    articles_text = json.load(f)\n"
      ],
      "metadata": {
        "id": "Zs245h-OpbHs"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# we tokenize our text data with their relevant tags from the dictionary\n",
        "\n",
        "tagged_arts = []\n",
        "for period, articles in articles_text.items():\n",
        "    for article in articles:\n",
        "        tag = period\n",
        "        tagged_arts.append(TaggedDocument(words=article.lower().split(), tags=[tag]))\n",
        "\n",
        "#\n",
        "# here we train our Doc2Vec model on every word of every article with attention to tags\n",
        "\n",
        "st = time.time()\n",
        "print('Running doc2vec, maybe consider getting the google stuff')\n",
        "doc2vec_model = Doc2Vec(tagged_arts, vector_size=256, min_count=10, epochs=100, workers=2)\n",
        "en = time.time()\n",
        "print('Time elapsed: ', en-st)\n",
        "\n",
        "doc2vec_model.save('/content/drive/MyDrive/Tralgo articles/d2v_M.model')\n"
      ],
      "metadata": {
        "id": "LjMC-uzm8lk1",
        "outputId": "c7463d6b-f155-484a-db06-49e6d06a0f60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running doc2vec, maybe consider getting the google stuff\n",
            "Time elapsed:  862.9126558303833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2vec_model = Doc2Vec.load('/content/drive/MyDrive/Tralgo articles/d2v_M.model')\n",
        "#\n",
        "# next we'll work on the embeddings a bit\n",
        "\n",
        "doc_embeds = {}\n",
        "for period, embeds in articles_text.items():\n",
        "    doc_embeds[period] = doc2vec_model.dv[period]\n",
        "\n",
        "doc_embeds_tens = {}\n",
        "for period, embeds in doc_embeds.items():\n",
        "    doc_embeds_tens[period] = np.array(embeds)\n",
        "\n",
        "#\n",
        "# in this part we transform words into vectors\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = {period: vectorizer.fit_transform(articles).toarray().astype(np.float32).flatten() for period, articles in articles_text.items()}\n",
        "\n",
        "#\n",
        "# and now we combine document embeddings with TF-IDF vectors for each period's articles\n",
        "\n",
        "X_data = {}\n",
        "for period in articles_text.keys():\n",
        "    embeds_vecs = np.concatenate((vectors[period], doc_embeds_tens[period]), axis=0)\n",
        "    X_data[period] = torch.tensor(embeds_vecs, dtype=torch.float32)\n",
        "\n",
        "#\n",
        "# pad the tensors to the same length before creating the dataframe\n",
        "\n",
        "padding = pad_sequence(list(X_data.values()), batch_first=True)\n",
        "\n",
        "#\n",
        "# here we create a date function to sort our index similarly in X and Y\n",
        "\n",
        "def Datelist(input_dates):\n",
        "    datelist = list(input_dates)\n",
        "    datelist.sort(key=lambda date: datetime.strptime(date.zfill(2), \"%Y-%m-B%d\"))\n",
        "    datelist = [re.sub(r'(?<=\\d{4}-)\\d{1}(?=-)', lambda match: match.group(0).zfill(2), date) for date in datelist]\n",
        "    return datelist\n",
        "\n",
        "#\n",
        "# create our X data\n",
        "\n",
        "X = pd.DataFrame(padding.numpy(), index=Datelist(articles_text.keys()), columns=[period for period in range(padding.shape[1])])\n",
        "feat_names = vectorizer.get_feature_names_out()\n",
        "X.columns = feat_names.tolist() + [f\"col_{i}\" for i in range(len(feat_names), len(X.columns))]\n",
        "X.index.name = \"Date\"\n",
        "\n",
        "#\n",
        "# create our Y data and intersect our datasets\n",
        "\n",
        "Y_data = feather.read_feather('/content/drive/MyDrive/Tralgo articles/financial_container.csv')\n",
        "Y = pd.DataFrame(Y_data['GOOG_future_indicator'], index=Datelist(Y_data.index))\n",
        "Y.index.name = \"Date\"\n",
        "\n",
        "tot_df = pd.merge(X, Y, how='inner', on='Date')\n",
        "X = tot_df.iloc[0:-1,0:-1]\n",
        "Y = tot_df.iloc[0:-1,-1]\n",
        "\n",
        "#\n",
        "# now we'll do some preprocessing of our data\n",
        "\n",
        "mm_scaler = preprocessing.MinMaxScaler()\n",
        "X_scale = mm_scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_val_test, Y_train, Y_val_test = train_test_split(X_scale, Y, test_size=0.2)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_val_test, Y_val_test, test_size=0.5)\n",
        "\n",
        "#\n",
        "# after this point we begin to code the neural network!\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.l1 = nn.Linear(len(X_train[0]), 64)\n",
        "        self.l2 = nn.Linear(64, 128)\n",
        "        self.l3 = nn.Linear(128, 64)\n",
        "        self.l4 = nn.Linear(64, 32)\n",
        "        self.l5 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.l1(x))\n",
        "        x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        x = self.relu(self.l2(x))\n",
        "        x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        x = self.relu(self.l3(x))\n",
        "        x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        x = self.relu(self.l4(x))\n",
        "        x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        x = self.sigmoid(self.l5(x))\n",
        "        return x\n",
        "\n",
        "dropout_prob = 0.3\n",
        "weight_decay = 0.01\n",
        "\n",
        "model = NeuralNet()\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), weight_decay=weight_decay)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "num_epochs = 100\n",
        "batch_size = 4\n",
        "best_val_loss = float('inf')\n",
        "patience = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    num_batches = len(X_train_tensor) // batch_size\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = (i + 1) * batch_size\n",
        "        inputs = X_train_tensor[start_idx:end_idx]\n",
        "        targets = Y_train_tensor[start_idx:end_idx]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Zero the gradients, backward pass, and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        val_loss = criterion(val_outputs, Y_val_tensor)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "    # Early stopping based on validation loss\n",
        "    if val_loss.item() < best_val_loss:\n",
        "        best_val_loss = val_loss.item()\n",
        "        patience = 10  # Reset patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        if patience == 0:\n",
        "            print(\"Early stopping...\")\n",
        "            break\n",
        "\n",
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/Tralgo articles/model.pt'))\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    test_accuracy = ((test_outputs >= 0.5).float() == Y_test_tensor).float().mean()\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy.item():.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Tralgo articles/model.pt')\n",
        "\n",
        "# if I ever want to load:\n",
        "# model.load_state_dict(torch.load('E:/Tralgo/model.pt'))"
      ],
      "metadata": {
        "id": "x7xbJKxr5n6K",
        "outputId": "1fedeaad-3a88-4910-a64c-f1fbf80bd6fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.5427, Val Loss: 0.7239\n",
            "Epoch [2/100], Loss: 0.6260, Val Loss: 0.6917\n",
            "Epoch [3/100], Loss: 0.6840, Val Loss: 0.6917\n",
            "Epoch [4/100], Loss: 0.5995, Val Loss: 0.6860\n",
            "Epoch [5/100], Loss: 0.6774, Val Loss: 0.6855\n",
            "Epoch [6/100], Loss: 0.7608, Val Loss: 0.6918\n",
            "Epoch [7/100], Loss: 0.6398, Val Loss: 0.6869\n",
            "Epoch [8/100], Loss: 0.7543, Val Loss: 0.6816\n",
            "Epoch [9/100], Loss: 0.7073, Val Loss: 0.6841\n",
            "Epoch [10/100], Loss: 0.6864, Val Loss: 0.6854\n",
            "Epoch [11/100], Loss: 0.7231, Val Loss: 0.6871\n",
            "Epoch [12/100], Loss: 0.6109, Val Loss: 0.6809\n"
          ]
        }
      ]
    }
  ]
}