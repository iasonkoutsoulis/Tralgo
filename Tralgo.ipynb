{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1WvjkfSYy-Y7C6kqq9PIajBvRH28FqSTg",
      "authorship_tag": "ABX9TyO9x/h3HnAl0rNz/ritqpV9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iasonkoutsoulis/Tralgo/blob/develop/Tralgo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9198_m5mtMzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5333440e-a9bc-486c-c931-8694cdfc9a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# This is the algorithm I'll use to do automated trading.\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from datetime import datetime\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "\n",
        "def link_collect(soop):\n",
        "    all_linx = []\n",
        "    for link in soop.find_all('a'):\n",
        "        nlink = link.get('href')\n",
        "        all_linx.append(nlink)\n",
        "        all_linx = list(filter(lambda item: item is not None, all_linx))\n",
        "    return all_linx\n",
        "\n",
        "def year_collect(soop):\n",
        "    years = []\n",
        "    for timet in soop.find_all('time', {'class': 'fc-date-headline'}):\n",
        "        years.append(re.findall(r'\\d+', timet.string)[-1])\n",
        "    years = list(dict.fromkeys(years))\n",
        "    return years\n",
        "\n",
        "def tl_collect(all_links, years):\n",
        "    for yeart in years:\n",
        "        expr = r'https:\\/\\/www\\.theguardian\\.com\\/\\S+\\/' + yeart + r'\\/\\S+'\n",
        "        text_links = []\n",
        "        for link in all_links:\n",
        "            if re.search(r'/all$', link):\n",
        "                pass\n",
        "            elif re.search(expr, link):\n",
        "                text_links.append(link)\n",
        "    return text_links\n",
        "\n",
        "#\n",
        "# main script\n",
        "\n",
        "bimon_arts = dict()\n",
        "for page in range(287, 0, -1):\n",
        "    print(str(page))\n",
        "\n",
        "    #\n",
        "    # initialize using the front-page links\n",
        "\n",
        "    url = 'https://www.theguardian.com/business/stock-markets?page=' + str(page) # total pages = 287\n",
        "    html = requests.get(url).text\n",
        "    soup = bs(html, 'lxml')\n",
        "\n",
        "    #\n",
        "    # get all article links from the page we've opened (we use the year they include to identify them)\n",
        "\n",
        "    all_links = link_collect(soup)\n",
        "    years = year_collect(soup)\n",
        "    text_links = tl_collect(all_links, years)\n",
        "\n",
        "    #\n",
        "    # now we open all of the articles on the page and collect them into our bimonthly datasets\n",
        "    # we create a dictionary/log entry which holds all text for a span of 15 days.\n",
        "\n",
        "    for tlink in text_links:\n",
        "        subhtml = requests.get(tlink).text\n",
        "        subsoup = bs(subhtml, 'lxml')\n",
        "        texpr = r'^.*?(?= \\||$)'\n",
        "        try:\n",
        "          title = re.search(texpr, subsoup.title.string).group(0)\n",
        "        except Exception:\n",
        "          pass\n",
        "\n",
        "        timet = subsoup.find('meta', {'property':'article:published_time'})\n",
        "        try:\n",
        "            fdate = timet['content']\n",
        "        except Exception:\n",
        "            pass\n",
        "        dt_date = datetime.strptime(fdate, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "        art_date = str(dt_date.year) + '-' + str(dt_date.month)\n",
        "        bimon = 'B2' if dt_date.day >= 15 else 'B1'\n",
        "\n",
        "        if not (art_date + '-' + bimon) in bimon_arts:\n",
        "            bimon_arts[art_date + '-' + bimon] = []\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        article = [title]\n",
        "        for textlink in subsoup.find_all('p'):\n",
        "            article.append(textlink.string)\n",
        "            article = list(filter(lambda item: item is not None, article))\n",
        "            art_str = \" \".join(article)\n",
        "        if art_str in bimon_arts[art_date + '-' + bimon]:\n",
        "            pass\n",
        "        else:\n",
        "            bimon_arts[art_date + '-' + bimon].append(art_str)\n",
        "\n",
        "#\n",
        "# instead of text files I'll try the JSON stuff now\n",
        "\n",
        "with open('/content/drive/MyDrive/Tralgo articles/article_container.json', \"w\") as f:\n",
        "    json.dump(bimon_arts, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this fetches the financial data and tests for stationarity\n",
        "import numpy as np\n",
        "import pyarrow.feather as feather\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "import yfinance as yf\n",
        "\n",
        "tics = 'msft aapl goog tsla'\n",
        "\n",
        "df = yf.download(tics, interval = \"1wk\", start='2008-10-01')\n",
        "df = df[['Adj Close']].dropna()\n",
        "df.columns = df.columns.droplevel()\n",
        "df.index = df.index.strftime('%Y-%m') + '-' + np.where(df.index.day>=15, 'B2', 'B1')\n",
        "df = df.groupby(df.index).mean()\n",
        "for col in df:\n",
        "    df[col + '_dif'] = df[col].diff()\n",
        "    df[col + '_indicator'] = np.where(df[col + '_dif'] >=0, 1, -1)\n",
        "    df[col + '_future_indicator'] = df[col + '_indicator'].shift(-1)\n",
        "\n",
        "adfuller(df['GOOG_dif'].dropna())\n",
        "\n",
        "feather.write_feather(df, '/content/drive/MyDrive/Tralgo articles/financial_container.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1MSllihpQC9",
        "outputId": "fc8b7d28-f0fc-48dc-a168-19a388ca82f6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%***********************]  4 of 4 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "with the present we'll train the text data along with the financial data and get our trained net.\n",
        "this will require some manipulation of the current state of the data, which makes sense that is done here,\n",
        "in order to promote homogeneity and peace of mind...\n",
        "'''\n",
        "from datetime import datetime\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow.feather as feather\n",
        "import re\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "\n",
        "try:\n",
        "    torch.cuda.get_device_name(0)\n",
        "except Exception:\n",
        "    print('Warning: \\nTorch not compiled with CUDA enabled.\\nRunning on CPU ...')\n",
        "\n",
        "#\n",
        "# Fetch articles data\n",
        "\n",
        "with open('/content/drive/MyDrive/Tralgo articles/article_container.json', 'r') as f:\n",
        "    articles_text = json.load(f)\n",
        "\n",
        "# #\n",
        "# # this is something we only do for the local code to run\n",
        "\n",
        "# import random\n",
        "# random.seed(None)\n",
        "# articles_text = dict(random.sample(list(articles_text.items()), 28))\n",
        "\n",
        "# #\n",
        "# # we tokenize our text data with their relevant tags from the dictionary\n",
        "\n",
        "# tagged_arts = []\n",
        "# for period, articles in articles_text.items():\n",
        "#     for article in articles:\n",
        "#         tag = period\n",
        "#         tagged_arts.append(TaggedDocument(words=article.lower().split(), tags=[tag]))\n",
        "\n",
        "# #\n",
        "# # here we train our Doc2Vec model on every word of every article with attention to tags\n",
        "# # note that vector_size dictates how many embeddings will pass into the X_data\n",
        "# # on colab, this model is trained for 1000 epochs.\n",
        "\n",
        "# # st = time.time()\n",
        "# # print('Running doc2vec, maybe consider getting the google stuff')\n",
        "# # doc2vec_model = Doc2Vec(tagged_arts, vector_size=128, min_count=10, epochs=10)\n",
        "# # en = time.time()\n",
        "# # print('Time elapsed: ', en-st)\n",
        "\n",
        "# # doc2vec_model.save('/content/drive/MyDrive/Tralgo articles/d2v_M.model')\n",
        "\n",
        "# #\n",
        "# # next we'll work on the embeddings a bit\n",
        "# # this d2v model is trained on Colab and downloaded here\n",
        "\n",
        "# doc2vec_model = Doc2Vec.load('/content/drive/MyDrive/Tralgo articles/d2v_M.model')\n",
        "\n",
        "# doc_embeds = {}\n",
        "# for period, embeds in articles_text.items():\n",
        "#     doc_embeds[period] = doc2vec_model.dv[period]\n",
        "\n",
        "# doc_embeds_tens = {}\n",
        "# for period, embeds in doc_embeds.items():\n",
        "#     doc_embeds_tens[period] = np.array(embeds)\n",
        "\n",
        "#\n",
        "# in this part we transform words into vectors\n",
        "\n",
        "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
        "vectors = {period: vectorizer.fit_transform(articles).toarray().astype(np.float32).flatten() for period, articles in articles_text.items()}\n",
        "\n",
        "#\n",
        "# and now we combine document embeddings with TF-IDF vectors for each period's articles\n",
        "\n",
        "X_data = {}\n",
        "for period in articles_text.keys():\n",
        "    # embeds_vecs = np.concatenate((vectors[period], doc_embeds_tens[period]), axis=0)\n",
        "    X_data[period] = torch.tensor(vectors[period], dtype=torch.float32)\n",
        "\n",
        "#\n",
        "# pad the tensors to the same length before creating the dataframe\n",
        "\n",
        "padding = pad_sequence(list(X_data.values()), batch_first=True)\n",
        "\n",
        "#\n",
        "# here we create a date function to sort our index similarly in X and Y\n",
        "\n",
        "def Datelist(input_dates):\n",
        "    datelist = list(input_dates)\n",
        "    datelist.sort(key=lambda date: datetime.strptime(date, \"%Y-%m-B%d\"))\n",
        "    datelist = [re.sub(r'(?<=\\d{4}-)\\d{1}(?=-)', lambda match: match.group(0).zfill(2), date) for date in datelist]\n",
        "    return datelist\n",
        "\n",
        "#\n",
        "# create our X data\n",
        "\n",
        "X = pd.DataFrame(padding.numpy(), index=Datelist(articles_text.keys()), columns=[period for period in range(padding.shape[1])])\n",
        "# feat_names = vectorizer.get_feature_names_out()\n",
        "# X.columns = feat_names.tolist() + [f\"col_{i}\" for i in range(len(feat_names), len(X.columns))]\n",
        "X.index.name = \"Date\"\n",
        "\n",
        "#\n",
        "# create our Y data and intersect our datasets\n",
        "\n",
        "Y_data = feather.read_feather('/content/drive/MyDrive/Tralgo articles/financial_container.csv')\n",
        "Y = pd.DataFrame(Y_data['GOOG_future_indicator'], index=Datelist(Y_data.index))\n",
        "Y.index.name = \"Date\"\n",
        "\n",
        "#\n",
        "# we leave out the last row because Y is NaN and we'll use it for prediction.\n",
        "\n",
        "tot_df = pd.merge(X, Y, how='inner', on='Date')\n",
        "X = tot_df.iloc[0:-1,0:-1]\n",
        "Y = tot_df.iloc[0:-1,-1]\n",
        "\n",
        "#\n",
        "# now we'll do some preprocessing of our data\n",
        "\n",
        "mm_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
        "X_scale = mm_scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_val_test, Y_train, Y_val_test = train_test_split(X_scale, Y, test_size=0.2)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_val_test, Y_val_test, test_size=0.5)\n",
        "\n",
        "#\n",
        "# after this point we begin to code the neural network!\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.l1 = nn.Linear(len(X_train[0]), 256)\n",
        "        # self.l2 = nn.Linear(64, 64)\n",
        "        # self.l3 = nn.Linear(64, 64)\n",
        "        # self.l4 = nn.Linear(64, 64)\n",
        "        # self.l5 = nn.Linear(64, 64)\n",
        "        self.l6 = nn.Linear(256, 128)\n",
        "        self.l7 = nn.Linear(128, 64)\n",
        "        self.l8 = nn.Linear(64, 2)\n",
        "        self.l9 = nn.Linear(2, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.l1(x))\n",
        "        # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        # x = self.relu(self.l2(x))\n",
        "        # # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        # x = self.relu(self.l3(x))\n",
        "        # # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        # x = self.relu(self.l4(x))\n",
        "        # # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        # x = self.relu(self.l5(x))\n",
        "        # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        x = self.relu(self.l6(x))\n",
        "        # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        x = self.relu(self.l7(x))\n",
        "        # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        x = self.relu(self.l8(x))\n",
        "        # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        x = self.sigmoid(self.l9(x))\n",
        "        return x\n",
        "\n",
        "dropout_prob = 0.0\n",
        "weight_decay = 0.01\n",
        "\n",
        "model = NeuralNet()\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "num_epochs = 300\n",
        "batch_size = 32\n",
        "best_val_loss = float('inf')\n",
        "best_loss = float('inf')\n",
        "patience = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    num_batches = len(X_train_tensor) // batch_size\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = (i + 1) * batch_size\n",
        "        inputs = X_train_tensor[start_idx:end_idx]\n",
        "        targets = Y_train_tensor[start_idx:end_idx]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Zero the gradients, backward pass, and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        val_loss = criterion(val_outputs, Y_val_tensor)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "    # Early stopping based on validation loss\n",
        "    if (val_loss.item() < best_val_loss) & (loss.item() < best_loss):\n",
        "        best_val_loss = val_loss.item()\n",
        "        best_loss = loss.item()\n",
        "        patience = 10  # Reset patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        if patience == 0:\n",
        "            print(\"Early stopping...\")\n",
        "            break\n",
        "\n",
        "#\n",
        "# if I ever want to load:\n",
        "# remove the training part of the models\n",
        "# ...\n",
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/Tralgo articles/model.pt'))\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    test_accuracy = ((test_outputs >= 0.0).float() == Y_test_tensor).float().mean()\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy.item():.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Tralgo articles/model.pt')"
      ],
      "metadata": {
        "id": "FFZOiXGw8YDx",
        "outputId": "1dd8945d-5d5c-467b-ffb0-c890ffa2460a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/300], Loss: 12.5000, Val Loss: 16.1290\n",
            "Epoch [2/300], Loss: 12.5000, Val Loss: 16.1290\n",
            "Epoch [3/300], Loss: 8.5854, Val Loss: 9.2606\n",
            "Epoch [4/300], Loss: 0.5238, Val Loss: 0.5439\n",
            "Epoch [5/300], Loss: 0.5234, Val Loss: 0.5434\n",
            "Epoch [6/300], Loss: 0.5223, Val Loss: 0.5424\n",
            "Epoch [7/300], Loss: 0.5209, Val Loss: 0.5412\n",
            "Epoch [8/300], Loss: 0.5195, Val Loss: 0.5400\n",
            "Epoch [9/300], Loss: 0.5181, Val Loss: 0.5388\n",
            "Epoch [10/300], Loss: 0.5168, Val Loss: 0.5377\n",
            "Epoch [11/300], Loss: 0.5154, Val Loss: 0.5366\n",
            "Epoch [12/300], Loss: 0.5142, Val Loss: 0.5355\n",
            "Epoch [13/300], Loss: 0.5129, Val Loss: 0.5345\n",
            "Epoch [14/300], Loss: 0.5117, Val Loss: 0.5335\n",
            "Epoch [15/300], Loss: 0.5106, Val Loss: 0.5325\n",
            "Epoch [16/300], Loss: 0.5094, Val Loss: 0.5315\n",
            "Epoch [17/300], Loss: 0.5083, Val Loss: 0.5306\n",
            "Epoch [18/300], Loss: 0.5072, Val Loss: 0.5297\n",
            "Epoch [19/300], Loss: 0.5061, Val Loss: 0.5288\n",
            "Epoch [20/300], Loss: 0.5051, Val Loss: 0.5279\n",
            "Epoch [21/300], Loss: 0.5041, Val Loss: 0.5271\n",
            "Epoch [22/300], Loss: 0.5031, Val Loss: 0.5262\n",
            "Epoch [23/300], Loss: 0.5021, Val Loss: 0.5254\n",
            "Epoch [24/300], Loss: 0.5011, Val Loss: 0.5246\n",
            "Epoch [25/300], Loss: 0.5001, Val Loss: 0.5238\n",
            "Epoch [26/300], Loss: 0.4992, Val Loss: 0.5230\n",
            "Epoch [27/300], Loss: 0.4983, Val Loss: 0.5222\n",
            "Epoch [28/300], Loss: 0.4973, Val Loss: 0.5215\n",
            "Epoch [29/300], Loss: 0.4964, Val Loss: 0.5207\n",
            "Epoch [30/300], Loss: 0.4956, Val Loss: 0.5200\n",
            "Epoch [31/300], Loss: 0.4947, Val Loss: 0.5193\n",
            "Epoch [32/300], Loss: 0.4938, Val Loss: 0.5186\n",
            "Epoch [33/300], Loss: 0.4930, Val Loss: 0.5179\n",
            "Epoch [34/300], Loss: 0.4922, Val Loss: 0.5172\n",
            "Epoch [35/300], Loss: 0.4914, Val Loss: 0.5165\n",
            "Epoch [36/300], Loss: 0.4906, Val Loss: 0.5159\n",
            "Epoch [37/300], Loss: 0.4898, Val Loss: 0.5152\n",
            "Epoch [38/300], Loss: 0.4890, Val Loss: 0.5146\n",
            "Epoch [39/300], Loss: 0.4882, Val Loss: 0.5140\n",
            "Epoch [40/300], Loss: 0.4875, Val Loss: 0.5133\n",
            "Epoch [41/300], Loss: 0.4867, Val Loss: 0.5127\n",
            "Epoch [42/300], Loss: 0.4860, Val Loss: 0.5121\n",
            "Epoch [43/300], Loss: 0.4853, Val Loss: 0.5115\n",
            "Epoch [44/300], Loss: 0.4845, Val Loss: 0.5110\n",
            "Epoch [45/300], Loss: 0.4838, Val Loss: 0.5104\n",
            "Epoch [46/300], Loss: 0.4832, Val Loss: 0.5098\n",
            "Epoch [47/300], Loss: 0.4825, Val Loss: 0.5093\n",
            "Epoch [48/300], Loss: 0.4818, Val Loss: 0.5087\n",
            "Epoch [49/300], Loss: 0.4811, Val Loss: 0.5082\n",
            "Epoch [50/300], Loss: 0.4805, Val Loss: 0.5076\n",
            "Epoch [51/300], Loss: 0.4798, Val Loss: 0.5071\n",
            "Epoch [52/300], Loss: 0.4792, Val Loss: 0.5066\n",
            "Epoch [53/300], Loss: 0.4786, Val Loss: 0.5061\n",
            "Epoch [54/300], Loss: 0.4780, Val Loss: 0.5056\n",
            "Epoch [55/300], Loss: 0.4773, Val Loss: 0.5051\n",
            "Epoch [56/300], Loss: 0.4767, Val Loss: 0.5046\n",
            "Epoch [57/300], Loss: 0.4762, Val Loss: 0.5041\n",
            "Epoch [58/300], Loss: 0.4756, Val Loss: 0.5037\n",
            "Epoch [59/300], Loss: 0.4750, Val Loss: 0.5032\n",
            "Epoch [60/300], Loss: 0.4744, Val Loss: 0.5027\n",
            "Epoch [61/300], Loss: 0.4739, Val Loss: 0.5023\n",
            "Epoch [62/300], Loss: 0.4733, Val Loss: 0.5019\n",
            "Epoch [63/300], Loss: 0.4728, Val Loss: 0.5014\n",
            "Epoch [64/300], Loss: 0.4722, Val Loss: 0.5010\n",
            "Epoch [65/300], Loss: 0.4717, Val Loss: 0.5006\n",
            "Epoch [66/300], Loss: 0.4712, Val Loss: 0.5001\n",
            "Epoch [67/300], Loss: 0.4707, Val Loss: 0.4997\n",
            "Epoch [68/300], Loss: 0.4702, Val Loss: 0.4993\n",
            "Epoch [69/300], Loss: 0.4697, Val Loss: 0.4989\n",
            "Epoch [70/300], Loss: 0.4692, Val Loss: 0.4985\n",
            "Epoch [71/300], Loss: 0.4687, Val Loss: 0.4981\n",
            "Epoch [72/300], Loss: 0.4682, Val Loss: 0.4978\n",
            "Epoch [73/300], Loss: 0.4677, Val Loss: 0.4974\n",
            "Epoch [74/300], Loss: 0.4673, Val Loss: 0.4970\n",
            "Epoch [75/300], Loss: 0.4668, Val Loss: 0.4966\n",
            "Epoch [76/300], Loss: 0.4663, Val Loss: 0.4963\n",
            "Epoch [77/300], Loss: 0.4659, Val Loss: 0.4959\n",
            "Epoch [78/300], Loss: 0.4655, Val Loss: 0.4956\n",
            "Epoch [79/300], Loss: 0.4650, Val Loss: 0.4952\n",
            "Epoch [80/300], Loss: 0.4646, Val Loss: 0.4947\n",
            "Epoch [81/300], Loss: 0.4642, Val Loss: 0.4945\n",
            "Epoch [82/300], Loss: 0.4637, Val Loss: 0.4942\n",
            "Epoch [83/300], Loss: 0.4633, Val Loss: 0.4939\n",
            "Epoch [84/300], Loss: 0.4629, Val Loss: 0.4936\n",
            "Epoch [85/300], Loss: 0.4625, Val Loss: 0.4932\n",
            "Epoch [86/300], Loss: 0.4621, Val Loss: 0.4929\n",
            "Epoch [87/300], Loss: 0.4617, Val Loss: 0.4926\n",
            "Epoch [88/300], Loss: 0.4614, Val Loss: 0.4923\n",
            "Epoch [89/300], Loss: 0.4610, Val Loss: 0.4920\n",
            "Epoch [90/300], Loss: 0.4606, Val Loss: 0.4917\n",
            "Epoch [91/300], Loss: 0.4602, Val Loss: 0.4914\n",
            "Epoch [92/300], Loss: 0.4599, Val Loss: 0.4911\n",
            "Epoch [93/300], Loss: 0.4595, Val Loss: 0.4909\n",
            "Epoch [94/300], Loss: 0.4591, Val Loss: 0.4906\n",
            "Epoch [95/300], Loss: 0.4588, Val Loss: 0.4903\n",
            "Epoch [96/300], Loss: 0.4584, Val Loss: 0.4900\n",
            "Epoch [97/300], Loss: 0.4581, Val Loss: 0.4898\n",
            "Epoch [98/300], Loss: 0.4578, Val Loss: 0.4895\n",
            "Epoch [99/300], Loss: 0.4574, Val Loss: 0.4892\n",
            "Epoch [100/300], Loss: 0.4571, Val Loss: 0.4890\n",
            "Epoch [101/300], Loss: 0.4568, Val Loss: 0.4887\n",
            "Epoch [102/300], Loss: 0.4565, Val Loss: 0.4885\n",
            "Epoch [103/300], Loss: 0.4561, Val Loss: 0.4882\n",
            "Epoch [104/300], Loss: 0.4558, Val Loss: 0.4880\n",
            "Epoch [105/300], Loss: 0.4555, Val Loss: 0.4877\n",
            "Epoch [106/300], Loss: 0.4552, Val Loss: 0.4875\n",
            "Epoch [107/300], Loss: 0.4549, Val Loss: 0.4873\n",
            "Epoch [108/300], Loss: 0.4546, Val Loss: 0.4870\n",
            "Epoch [109/300], Loss: 0.4543, Val Loss: 0.4868\n",
            "Epoch [110/300], Loss: 0.4541, Val Loss: 0.4866\n",
            "Epoch [111/300], Loss: 0.4538, Val Loss: 0.4864\n",
            "Epoch [112/300], Loss: 0.4535, Val Loss: 0.4862\n",
            "Epoch [113/300], Loss: 0.4532, Val Loss: 0.4859\n",
            "Epoch [114/300], Loss: 0.4529, Val Loss: 0.4857\n",
            "Epoch [115/300], Loss: 0.4527, Val Loss: 0.4855\n",
            "Epoch [116/300], Loss: 0.4524, Val Loss: 0.4853\n",
            "Epoch [117/300], Loss: 0.4521, Val Loss: 0.4851\n",
            "Epoch [118/300], Loss: 0.4519, Val Loss: 0.4849\n",
            "Epoch [119/300], Loss: 0.4516, Val Loss: 0.4847\n",
            "Epoch [120/300], Loss: 0.4514, Val Loss: 0.4845\n",
            "Epoch [121/300], Loss: 0.4511, Val Loss: 0.4843\n",
            "Epoch [122/300], Loss: 0.4509, Val Loss: 0.4841\n",
            "Epoch [123/300], Loss: 0.4506, Val Loss: 0.4840\n",
            "Epoch [124/300], Loss: 0.4504, Val Loss: 0.4838\n",
            "Epoch [125/300], Loss: 0.4502, Val Loss: 0.4836\n",
            "Epoch [126/300], Loss: 0.4499, Val Loss: 0.4834\n",
            "Epoch [127/300], Loss: 0.4497, Val Loss: 0.4832\n",
            "Epoch [128/300], Loss: 0.4495, Val Loss: 0.4831\n",
            "Epoch [129/300], Loss: 0.4493, Val Loss: 0.4823\n",
            "Epoch [130/300], Loss: 0.4490, Val Loss: 0.4827\n",
            "Epoch [131/300], Loss: 0.4488, Val Loss: 0.4826\n",
            "Epoch [132/300], Loss: 0.4486, Val Loss: 0.4824\n",
            "Epoch [133/300], Loss: 0.4484, Val Loss: 0.4822\n",
            "Epoch [134/300], Loss: 0.4482, Val Loss: 0.4821\n",
            "Epoch [135/300], Loss: 0.4480, Val Loss: 0.4819\n",
            "Epoch [136/300], Loss: 0.4478, Val Loss: 0.4817\n",
            "Epoch [137/300], Loss: 0.4476, Val Loss: 0.4816\n",
            "Epoch [138/300], Loss: 0.4474, Val Loss: 0.4814\n",
            "Epoch [139/300], Loss: 0.4472, Val Loss: 0.4813\n",
            "Epoch [140/300], Loss: 0.4470, Val Loss: 0.4811\n",
            "Epoch [141/300], Loss: 0.4468, Val Loss: 0.4810\n",
            "Epoch [142/300], Loss: 0.4466, Val Loss: 0.4809\n",
            "Epoch [143/300], Loss: 0.4464, Val Loss: 0.4807\n",
            "Epoch [144/300], Loss: 0.4462, Val Loss: 0.4806\n",
            "Epoch [145/300], Loss: 0.4461, Val Loss: 0.4804\n",
            "Epoch [146/300], Loss: 0.4459, Val Loss: 0.4803\n",
            "Epoch [147/300], Loss: 0.4457, Val Loss: 0.4802\n",
            "Epoch [148/300], Loss: 0.4455, Val Loss: 0.4800\n",
            "Epoch [149/300], Loss: 0.4454, Val Loss: 0.4799\n",
            "Epoch [150/300], Loss: 0.4452, Val Loss: 0.4798\n",
            "Epoch [151/300], Loss: 0.4450, Val Loss: 0.4796\n",
            "Epoch [152/300], Loss: 0.4449, Val Loss: 0.4795\n",
            "Epoch [153/300], Loss: 0.4447, Val Loss: 0.4794\n",
            "Epoch [154/300], Loss: 0.4445, Val Loss: 0.4793\n",
            "Epoch [155/300], Loss: 0.4443, Val Loss: 0.4792\n",
            "Epoch [156/300], Loss: 0.4442, Val Loss: 0.4790\n",
            "Epoch [157/300], Loss: 0.4441, Val Loss: 0.4789\n",
            "Epoch [158/300], Loss: 0.4439, Val Loss: 0.4788\n",
            "Epoch [159/300], Loss: 0.4438, Val Loss: 0.4787\n",
            "Epoch [160/300], Loss: 0.4436, Val Loss: 0.4786\n",
            "Epoch [161/300], Loss: 0.4435, Val Loss: 0.4785\n",
            "Epoch [162/300], Loss: 0.4433, Val Loss: 0.4784\n",
            "Epoch [163/300], Loss: 0.4432, Val Loss: 0.4782\n",
            "Epoch [164/300], Loss: 0.4430, Val Loss: 0.4781\n",
            "Epoch [165/300], Loss: 0.4429, Val Loss: 0.4780\n",
            "Epoch [166/300], Loss: 0.4428, Val Loss: 0.4779\n",
            "Epoch [167/300], Loss: 0.4426, Val Loss: 0.4778\n",
            "Epoch [168/300], Loss: 0.4425, Val Loss: 0.4777\n",
            "Epoch [169/300], Loss: 0.4424, Val Loss: 0.4776\n",
            "Epoch [170/300], Loss: 0.4422, Val Loss: 0.4775\n",
            "Epoch [171/300], Loss: 0.4421, Val Loss: 0.4774\n",
            "Epoch [172/300], Loss: 0.4420, Val Loss: 0.4773\n",
            "Epoch [173/300], Loss: 0.4419, Val Loss: 0.4772\n",
            "Epoch [174/300], Loss: 0.4417, Val Loss: 0.4772\n",
            "Epoch [175/300], Loss: 0.4416, Val Loss: 0.4771\n",
            "Epoch [176/300], Loss: 0.4415, Val Loss: 0.4770\n",
            "Epoch [177/300], Loss: 0.4414, Val Loss: 0.4769\n",
            "Epoch [178/300], Loss: 0.4413, Val Loss: 0.4768\n",
            "Epoch [179/300], Loss: 0.4410, Val Loss: 0.4767\n",
            "Epoch [180/300], Loss: 0.4410, Val Loss: 0.4766\n",
            "Epoch [181/300], Loss: 0.4409, Val Loss: 0.4765\n",
            "Epoch [182/300], Loss: 0.4408, Val Loss: 0.4764\n",
            "Epoch [183/300], Loss: 0.4407, Val Loss: 0.4764\n",
            "Epoch [184/300], Loss: 0.4406, Val Loss: 0.4763\n",
            "Epoch [185/300], Loss: 0.4405, Val Loss: 0.4762\n",
            "Epoch [186/300], Loss: 0.4404, Val Loss: 0.4761\n",
            "Epoch [187/300], Loss: 0.4403, Val Loss: 0.4760\n",
            "Epoch [188/300], Loss: 0.4402, Val Loss: 0.4760\n",
            "Epoch [189/300], Loss: 0.4401, Val Loss: 0.4759\n",
            "Epoch [190/300], Loss: 0.4400, Val Loss: 0.4758\n",
            "Epoch [191/300], Loss: 0.4399, Val Loss: 0.4757\n",
            "Epoch [192/300], Loss: 0.4398, Val Loss: 0.4757\n",
            "Epoch [193/300], Loss: 0.4397, Val Loss: 0.4756\n",
            "Epoch [194/300], Loss: 0.4396, Val Loss: 0.4755\n",
            "Epoch [195/300], Loss: 0.4395, Val Loss: 0.4754\n",
            "Epoch [196/300], Loss: 0.4394, Val Loss: 0.4754\n",
            "Epoch [197/300], Loss: 0.4393, Val Loss: 0.4753\n",
            "Epoch [198/300], Loss: 0.4392, Val Loss: 0.4752\n",
            "Epoch [199/300], Loss: 0.4391, Val Loss: 0.4752\n",
            "Epoch [200/300], Loss: 0.4390, Val Loss: 0.4751\n",
            "Epoch [201/300], Loss: 0.4389, Val Loss: 0.4750\n",
            "Epoch [202/300], Loss: 0.4388, Val Loss: 0.4750\n",
            "Epoch [203/300], Loss: 0.4388, Val Loss: 0.4749\n",
            "Epoch [204/300], Loss: 0.4387, Val Loss: 0.4748\n",
            "Epoch [205/300], Loss: 0.4386, Val Loss: 0.4748\n",
            "Epoch [206/300], Loss: 0.4385, Val Loss: 0.4747\n",
            "Epoch [207/300], Loss: 0.4384, Val Loss: 0.4747\n",
            "Epoch [208/300], Loss: 0.4383, Val Loss: 0.4745\n",
            "Epoch [209/300], Loss: 0.4383, Val Loss: 0.4745\n",
            "Epoch [210/300], Loss: 0.4382, Val Loss: 0.4745\n",
            "Epoch [211/300], Loss: 0.4381, Val Loss: 0.4744\n",
            "Epoch [212/300], Loss: 0.4380, Val Loss: 0.4744\n",
            "Epoch [213/300], Loss: 0.4380, Val Loss: 0.4743\n",
            "Epoch [214/300], Loss: 0.4379, Val Loss: 0.4743\n",
            "Epoch [215/300], Loss: 0.4378, Val Loss: 0.4742\n",
            "Epoch [216/300], Loss: 0.4377, Val Loss: 0.4741\n",
            "Epoch [217/300], Loss: 0.4377, Val Loss: 0.4741\n",
            "Epoch [218/300], Loss: 0.4376, Val Loss: 0.4741\n",
            "Epoch [219/300], Loss: 0.4375, Val Loss: 0.4740\n",
            "Epoch [220/300], Loss: 0.4375, Val Loss: 0.4739\n",
            "Epoch [221/300], Loss: 0.4374, Val Loss: 0.4739\n",
            "Epoch [222/300], Loss: 0.4373, Val Loss: 0.4738\n",
            "Epoch [223/300], Loss: 0.4373, Val Loss: 0.4738\n",
            "Epoch [224/300], Loss: 0.4372, Val Loss: 0.4738\n",
            "Epoch [225/300], Loss: 0.4371, Val Loss: 0.4737\n",
            "Epoch [226/300], Loss: 0.4371, Val Loss: 0.4737\n",
            "Epoch [227/300], Loss: 0.4370, Val Loss: 0.4736\n",
            "Epoch [228/300], Loss: 0.4370, Val Loss: 0.4736\n",
            "Epoch [229/300], Loss: 0.4369, Val Loss: 0.4735\n",
            "Epoch [230/300], Loss: 0.4369, Val Loss: 0.4735\n",
            "Epoch [231/300], Loss: 0.4368, Val Loss: 0.4735\n",
            "Epoch [232/300], Loss: 0.4367, Val Loss: 0.4734\n",
            "Epoch [233/300], Loss: 0.4367, Val Loss: 0.4734\n",
            "Epoch [234/300], Loss: 0.4366, Val Loss: 0.4733\n",
            "Epoch [235/300], Loss: 0.4366, Val Loss: 0.4733\n",
            "Epoch [236/300], Loss: 0.4365, Val Loss: 0.4732\n",
            "Epoch [237/300], Loss: 0.4365, Val Loss: 0.4732\n",
            "Epoch [238/300], Loss: 0.4364, Val Loss: 0.4732\n",
            "Epoch [239/300], Loss: 0.4364, Val Loss: 0.4731\n",
            "Epoch [240/300], Loss: 0.4363, Val Loss: 0.4731\n",
            "Epoch [241/300], Loss: 0.4362, Val Loss: 0.4730\n",
            "Epoch [242/300], Loss: 0.4362, Val Loss: 0.4730\n",
            "Epoch [243/300], Loss: 0.4361, Val Loss: 0.4730\n",
            "Epoch [244/300], Loss: 0.4361, Val Loss: 0.4729\n",
            "Epoch [245/300], Loss: 0.4360, Val Loss: 0.4729\n",
            "Epoch [246/300], Loss: 0.4360, Val Loss: 0.4729\n",
            "Epoch [247/300], Loss: 0.4359, Val Loss: 0.4728\n",
            "Epoch [248/300], Loss: 0.4359, Val Loss: 0.4728\n",
            "Epoch [249/300], Loss: 0.4358, Val Loss: 0.4727\n",
            "Epoch [250/300], Loss: 0.4358, Val Loss: 0.4727\n",
            "Epoch [251/300], Loss: 0.4358, Val Loss: 0.4727\n",
            "Epoch [252/300], Loss: 0.4357, Val Loss: 0.4726\n",
            "Epoch [253/300], Loss: 0.4357, Val Loss: 0.4726\n",
            "Epoch [254/300], Loss: 0.4356, Val Loss: 0.4726\n",
            "Epoch [255/300], Loss: 0.4356, Val Loss: 0.4725\n",
            "Epoch [256/300], Loss: 0.4355, Val Loss: 0.4725\n",
            "Epoch [257/300], Loss: 0.4355, Val Loss: 0.4725\n",
            "Epoch [258/300], Loss: 0.4354, Val Loss: 0.4724\n",
            "Epoch [259/300], Loss: 0.4354, Val Loss: 0.4724\n",
            "Epoch [260/300], Loss: 0.4354, Val Loss: 0.4724\n",
            "Epoch [261/300], Loss: 0.4353, Val Loss: 0.4724\n",
            "Epoch [262/300], Loss: 0.4353, Val Loss: 0.4723\n",
            "Epoch [263/300], Loss: 0.4352, Val Loss: 0.4723\n",
            "Epoch [264/300], Loss: 0.4352, Val Loss: 0.4723\n",
            "Epoch [265/300], Loss: 0.4352, Val Loss: 0.4722\n",
            "Epoch [266/300], Loss: 0.4351, Val Loss: 0.4722\n",
            "Epoch [267/300], Loss: 0.4351, Val Loss: 0.4722\n",
            "Epoch [268/300], Loss: 0.4351, Val Loss: 0.4722\n",
            "Epoch [269/300], Loss: 0.4350, Val Loss: 0.4721\n",
            "Epoch [270/300], Loss: 0.4350, Val Loss: 0.4721\n",
            "Epoch [271/300], Loss: 0.4349, Val Loss: 0.4721\n",
            "Epoch [272/300], Loss: 0.4349, Val Loss: 0.4721\n",
            "Epoch [273/300], Loss: 0.4349, Val Loss: 0.4720\n",
            "Epoch [274/300], Loss: 0.4348, Val Loss: 0.4720\n",
            "Epoch [275/300], Loss: 0.4348, Val Loss: 0.4720\n",
            "Epoch [276/300], Loss: 0.4348, Val Loss: 0.4719\n",
            "Epoch [277/300], Loss: 0.4347, Val Loss: 0.4719\n",
            "Epoch [278/300], Loss: 0.4347, Val Loss: 0.4719\n",
            "Epoch [279/300], Loss: 0.4347, Val Loss: 0.4719\n",
            "Epoch [280/300], Loss: 0.4346, Val Loss: 0.4719\n",
            "Epoch [281/300], Loss: 0.4346, Val Loss: 0.4718\n",
            "Epoch [282/300], Loss: 0.4346, Val Loss: 0.4718\n",
            "Epoch [283/300], Loss: 0.4345, Val Loss: 0.4718\n",
            "Epoch [284/300], Loss: 0.4345, Val Loss: 0.4718\n",
            "Epoch [285/300], Loss: 0.4345, Val Loss: 0.4717\n",
            "Epoch [286/300], Loss: 0.4345, Val Loss: 0.4717\n",
            "Epoch [287/300], Loss: 0.4344, Val Loss: 0.4717\n",
            "Epoch [288/300], Loss: 0.4344, Val Loss: 0.4717\n",
            "Epoch [289/300], Loss: 0.4344, Val Loss: 0.4717\n",
            "Epoch [290/300], Loss: 0.4343, Val Loss: 0.4716\n",
            "Epoch [291/300], Loss: 0.4343, Val Loss: 0.4716\n",
            "Epoch [292/300], Loss: 0.4343, Val Loss: 0.4716\n",
            "Epoch [293/300], Loss: 0.4343, Val Loss: 0.4716\n",
            "Epoch [294/300], Loss: 0.4342, Val Loss: 0.4716\n",
            "Epoch [295/300], Loss: 0.4342, Val Loss: 0.4715\n",
            "Epoch [296/300], Loss: 0.4342, Val Loss: 0.4715\n",
            "Epoch [297/300], Loss: 0.4342, Val Loss: 0.4715\n",
            "Epoch [298/300], Loss: 0.4341, Val Loss: 0.4715\n",
            "Epoch [299/300], Loss: 0.4341, Val Loss: 0.4715\n",
            "Epoch [300/300], Loss: 0.4341, Val Loss: 0.4714\n",
            "Test Accuracy: 0.5312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle training/testing split based on epochs\n",
        "# make sure the padding works out properly\n",
        "# try because otherwise... more input"
      ],
      "metadata": {
        "id": "jbgBj4KyeE3O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}