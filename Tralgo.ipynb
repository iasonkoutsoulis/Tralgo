{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iasonkoutsoulis/Tralgo/blob/develop/Tralgo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "9198_m5mtMzA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "763329ec-c7f8-4452-a43a-ebfe9724d8b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "53\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-548682828.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_links\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0msubhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.theguardian.com'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0msubsoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'^.*?(?= \\||$)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1431\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# This is the algorithm I'll use to do automated trading.\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "\n",
        "def link_collect(soop):\n",
        "    all_linx = []\n",
        "    for link in soop.find_all('a'):\n",
        "        nlink = link.get('href')\n",
        "        all_linx.append(nlink)\n",
        "        all_linx = list(filter(lambda item: item is not None, all_linx))\n",
        "    return all_linx\n",
        "\n",
        "def year_collect(soop):\n",
        "    years = []\n",
        "    seen = set()\n",
        "\n",
        "    for t in soup.select('time[datetime]'):\n",
        "        y = t['datetime'][:4]\n",
        "        if y not in seen:\n",
        "            seen.add(y)\n",
        "            years.append(y)\n",
        "\n",
        "    return years\n",
        "\n",
        "def tl_collect(all_links, years):\n",
        "    for yeart in years:\n",
        "        expr = expr = re.compile(r'^/.*/' + re.escape(yeart) + r'/[^#\\s]+$')\n",
        "        text_links = []\n",
        "        for link in all_links:\n",
        "            if re.search(r'/all$', link):\n",
        "                pass\n",
        "            elif re.search(expr, link):\n",
        "                text_links.append(link)\n",
        "    return text_links\n",
        "\n",
        "#\n",
        "# main script\n",
        "\n",
        "bimon_arts = dict()\n",
        "for page in range(329, 0, -1):\n",
        "    print(str(page))\n",
        "\n",
        "    #\n",
        "    # initialize using the front-page links\n",
        "\n",
        "    url = 'https://www.theguardian.com/business/stock-markets?page=' + str(page) # total pages = 329 (as of 01-10-2025)\n",
        "    html = requests.get(url).text\n",
        "    soup = bs(html, 'lxml')\n",
        "\n",
        "    #\n",
        "    # get all article links from the page we've opened (we use the year they include to identify them)\n",
        "\n",
        "    all_links = link_collect(soup)\n",
        "    years = year_collect(soup)\n",
        "    text_links = tl_collect(all_links, years)\n",
        "\n",
        "    #\n",
        "    # now we open all of the articles on the page and collect them into our bimonthly datasets\n",
        "    # we create a dictionary/log entry which holds all text for a span of 15 days.\n",
        "\n",
        "    for tlink in text_links:\n",
        "        subhtml = requests.get('https://www.theguardian.com' + tlink).text\n",
        "        subsoup = bs(subhtml, 'lxml')\n",
        "        texpr = r'^.*?(?= \\||$)'\n",
        "        try:\n",
        "          title = re.search(texpr, subsoup.title.string).group(0)\n",
        "        except Exception:\n",
        "          pass\n",
        "\n",
        "        timet = subsoup.find('meta', {'property':'article:published_time'})\n",
        "        try:\n",
        "            fdate = timet['content']\n",
        "        except Exception:\n",
        "            pass\n",
        "        dt_date = datetime.strptime(fdate, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "        art_date = str(dt_date.year) + '-' + str(dt_date.month)\n",
        "        bimon = 'B2' if dt_date.day >= 15 else 'B1'\n",
        "\n",
        "        if not (art_date + '-' + bimon) in bimon_arts:\n",
        "            bimon_arts[art_date + '-' + bimon] = []\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        article = [title]\n",
        "        for textlink in subsoup.find_all('p'):\n",
        "            article.append(textlink.string)\n",
        "            article = list(filter(lambda item: item is not None, article))\n",
        "            art_str = \" \".join(article)\n",
        "        if art_str in bimon_arts[art_date + '-' + bimon]:\n",
        "            pass\n",
        "        else:\n",
        "            bimon_arts[art_date + '-' + bimon].append(art_str)\n",
        "\n",
        "#\n",
        "# instead of text files I'll try the JSON stuff now\n",
        "# os.makedirs('/content/drive/MyDrive/Tralgo articles', exist_ok=True)\n",
        "# with open('/content/drive/MyDrive/Tralgo articles/article_container.json', \"w\") as f:\n",
        "#     json.dump(bimon_arts, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this fetches the financial data and tests for stationarity\n",
        "import numpy as np\n",
        "import pyarrow.feather as feather\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "import yfinance as yf\n",
        "\n",
        "tics = 'msft aapl goog tsla'\n",
        "\n",
        "df = yf.download(tics, interval = \"1wk\", start='2008-10-01')\n",
        "df = df[['Close']].dropna()\n",
        "df.columns = df.columns.droplevel()\n",
        "df.index = df.index.strftime('%Y-%m') + '-' + np.where(df.index.day>=15, 'B2', 'B1')\n",
        "df = df.groupby(df.index).mean()\n",
        "for col in df:\n",
        "    df[col + '_dif'] = df[col].diff()\n",
        "    df[col + '_indicator'] = np.where(df[col + '_dif'] >=0, 1, 0)\n",
        "    df[col + '_future_indicator'] = df[col + '_indicator'].shift(-1)\n",
        "\n",
        "adfuller(df['GOOG_dif'].dropna())\n",
        "\n",
        "feather.write_feather(df, '/content/drive/MyDrive/Tralgo articles/financial_container.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1MSllihpQC9",
        "outputId": "e5e1bcbe-c54f-4fe1-c316-fed963ce7bf8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2425415928.py:9: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(tics, interval = \"1wk\", start='2008-10-01')\n",
            "[*********************100%***********************]  4 of 4 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "KwxLgh70kJJY",
        "outputId": "4e27a23a-2e00-4755-e13a-4cd78f7e76e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "74bf43c191ed47ca8df56acfc3a25a35"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "with the present we'll train the text data along with the financial data and get our trained net.\n",
        "this will require some manipulation of the current state of the data, which makes sense that is done here,\n",
        "in order to promote homogeneity and peace of mind...\n",
        "'''\n",
        "from datetime import datetime\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow.feather as feather\n",
        "import re\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "\n",
        "try:\n",
        "    torch.cuda.get_device_name(0)\n",
        "except Exception:\n",
        "    print('Warning: \\nTorch not compiled with CUDA enabled.\\nRunning on CPU ...')\n",
        "\n",
        "#\n",
        "# Fetch articles data\n",
        "\n",
        "with open('/content/drive/MyDrive/Tralgo articles/article_container.json', 'r') as f:\n",
        "    articles_text = json.load(f)\n",
        "\n",
        "# #\n",
        "# # this is something we only do for the local code to run\n",
        "\n",
        "# import random\n",
        "# random.seed(None)\n",
        "# articles_text = dict(random.sample(list(articles_text.items()), 28))\n",
        "\n",
        "# #\n",
        "# # we tokenize our text data with their relevant tags from the dictionary\n",
        "\n",
        "# tagged_arts = []\n",
        "# for period, articles in articles_text.items():\n",
        "#     for article in articles:\n",
        "#         tag = period\n",
        "#         tagged_arts.append(TaggedDocument(words=article.lower().split(), tags=[tag]))\n",
        "\n",
        "# #\n",
        "# # here we train our Doc2Vec model on every word of every article with attention to tags\n",
        "# # note that vector_size dictates how many embeddings will pass into the X_data\n",
        "# # on colab, this model is trained for 1000 epochs.\n",
        "\n",
        "# # st = time.time()\n",
        "# # print('Running doc2vec, maybe consider getting the google stuff')\n",
        "# # doc2vec_model = Doc2Vec(tagged_arts, vector_size=128, min_count=10, epochs=10)\n",
        "# # en = time.time()\n",
        "# # print('Time elapsed: ', en-st)\n",
        "\n",
        "# # doc2vec_model.save('/content/drive/MyDrive/Tralgo articles/d2v_M.model')\n",
        "\n",
        "# #\n",
        "# # next we'll work on the embeddings a bit\n",
        "# # this d2v model is trained on Colab and downloaded here\n",
        "\n",
        "# doc2vec_model = Doc2Vec.load('/content/drive/MyDrive/Tralgo articles/d2v_M.model')\n",
        "\n",
        "# doc_embeds = {}\n",
        "# for period, embeds in articles_text.items():\n",
        "#     doc_embeds[period] = doc2vec_model.dv[period]\n",
        "\n",
        "# doc_embeds_tens = {}\n",
        "# for period, embeds in doc_embeds.items():\n",
        "#     doc_embeds_tens[period] = np.array(embeds)\n",
        "\n",
        "#\n",
        "# in this part we transform words into vectors\n",
        "\n",
        "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
        "vectors = {period: vectorizer.fit_transform(articles).toarray().astype(np.float32).flatten() for period, articles in articles_text.items()}\n",
        "\n",
        "#\n",
        "# and now we combine document embeddings with TF-IDF vectors for each period's articles\n",
        "\n",
        "X_data = {}\n",
        "for period in articles_text.keys():\n",
        "    # embeds_vecs = np.concatenate((vectors[period], doc_embeds_tens[period]), axis=0)\n",
        "    X_data[period] = torch.tensor(vectors[period], dtype=torch.float32)\n",
        "\n",
        "#\n",
        "# pad the tensors to the same length before creating the dataframe\n",
        "\n",
        "padding = pad_sequence(list(X_data.values()), batch_first=True)\n",
        "\n",
        "#\n",
        "# here we create a date function to sort our index similarly in X and Y\n",
        "\n",
        "def Datelist(input_dates):\n",
        "    datelist = list(input_dates)\n",
        "    datelist.sort(key=lambda date: datetime.strptime(date, \"%Y-%m-B%d\"))\n",
        "    datelist = [re.sub(r'(?<=\\d{4}-)\\d{1}(?=-)', lambda match: match.group(0).zfill(2), date) for date in datelist]\n",
        "    return datelist\n",
        "\n",
        "#\n",
        "# create our X data\n",
        "\n",
        "X = pd.DataFrame(padding.numpy(), index=Datelist(articles_text.keys()), columns=[period for period in range(padding.shape[1])])\n",
        "# feat_names = vectorizer.get_feature_names_out()\n",
        "# X.columns = feat_names.tolist() + [f\"col_{i}\" for i in range(len(feat_names), len(X.columns))]\n",
        "X.index.name = \"Date\"\n",
        "\n",
        "#\n",
        "# create our Y data and intersect our datasets\n",
        "\n",
        "Y_data = feather.read_feather('/content/drive/MyDrive/Tralgo articles/financial_container.csv')\n",
        "Y = pd.DataFrame(Y_data['GOOG_future_indicator'], index=Datelist(Y_data.index))\n",
        "Y.index.name = \"Date\"\n",
        "\n",
        "#\n",
        "# we leave out the last row because Y is NaN and we'll use it for prediction.\n",
        "\n",
        "tot_df = pd.merge(X, Y, how='inner', on='Date')\n",
        "X = tot_df.iloc[0:-1,0:-1]\n",
        "Y = tot_df.iloc[0:-1,-1]\n",
        "\n",
        "#\n",
        "# now we'll do some preprocessing of our data\n",
        "\n",
        "mm_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
        "X_scale = mm_scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_val_test, Y_train, Y_val_test = train_test_split(X_scale, Y, test_size=0.2)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_val_test, Y_val_test, test_size=0.5)\n",
        "\n",
        "#\n",
        "# after this point we begin to code the neural network!\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.l1 = nn.Linear(len(X_train[0]), 256)\n",
        "        # self.l2 = nn.Linear(64, 64)\n",
        "        # self.l3 = nn.Linear(64, 64)\n",
        "        # self.l4 = nn.Linear(64, 64)\n",
        "        # self.l5 = nn.Linear(64, 64)\n",
        "        self.l6 = nn.Linear(256, 128)\n",
        "        self.l7 = nn.Linear(128, 64)\n",
        "        self.l8 = nn.Linear(64, 2)\n",
        "        self.l9 = nn.Linear(2, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.l1(x))\n",
        "        # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        # x = self.relu(self.l2(x))\n",
        "        # # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        # x = self.relu(self.l3(x))\n",
        "        # # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        # x = self.relu(self.l4(x))\n",
        "        # # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        # x = self.relu(self.l5(x))\n",
        "        # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        x = self.relu(self.l6(x))\n",
        "        # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        x = self.relu(self.l7(x))\n",
        "        # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        x = self.relu(self.l8(x))\n",
        "        # x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
        "        x = self.sigmoid(self.l9(x))\n",
        "        return x\n",
        "\n",
        "dropout_prob = 0.0\n",
        "weight_decay = 0.01\n",
        "\n",
        "model = NeuralNet()\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "num_epochs = 300\n",
        "batch_size = 32\n",
        "best_val_loss = float('inf')\n",
        "best_loss = float('inf')\n",
        "patience = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    num_batches = len(X_train_tensor) // batch_size\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = (i + 1) * batch_size\n",
        "        inputs = X_train_tensor[start_idx:end_idx]\n",
        "        targets = Y_train_tensor[start_idx:end_idx]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Zero the gradients, backward pass, and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        val_loss = criterion(val_outputs, Y_val_tensor)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "    # Early stopping based on validation loss\n",
        "    if (val_loss.item() < best_val_loss) & (loss.item() < best_loss):\n",
        "        best_val_loss = val_loss.item()\n",
        "        best_loss = loss.item()\n",
        "        patience = 10  # Reset patience\n",
        "    else:\n",
        "        patience -= 1\n",
        "        if patience == 0:\n",
        "            print(\"Early stopping...\")\n",
        "            break\n",
        "\n",
        "#\n",
        "# if I ever want to load:\n",
        "# remove the training part of the models\n",
        "# ...\n",
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/Tralgo articles/model.pt'))\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    test_accuracy = ((test_outputs >= 0.0).float() == Y_test_tensor).float().mean()\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy.item():.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Tralgo articles/model.pt')"
      ],
      "metadata": {
        "id": "FFZOiXGw8YDx",
        "outputId": "ef8c2e34-2ecf-4e86-b883-09842707a776",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3486418599.py:185: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32).view(-1, 1)\n",
            "/tmp/ipython-input-3486418599.py:187: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32).view(-1, 1)\n",
            "/tmp/ipython-input-3486418599.py:189: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).view(-1, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/300], Loss: 37.5000, Val Loss: 36.1111\n",
            "Epoch [2/300], Loss: 37.5000, Val Loss: 36.1111\n",
            "Epoch [3/300], Loss: 37.5000, Val Loss: 36.1111\n",
            "Epoch [4/300], Loss: 0.8442, Val Loss: 1.9469\n",
            "Epoch [5/300], Loss: 0.7668, Val Loss: 0.7724\n",
            "Epoch [6/300], Loss: 0.7646, Val Loss: 0.7699\n",
            "Epoch [7/300], Loss: 0.7621, Val Loss: 0.7673\n",
            "Epoch [8/300], Loss: 0.7597, Val Loss: 0.7648\n",
            "Epoch [9/300], Loss: 0.7574, Val Loss: 0.7623\n",
            "Epoch [10/300], Loss: 0.7552, Val Loss: 0.7600\n",
            "Epoch [11/300], Loss: 0.7530, Val Loss: 0.7577\n",
            "Epoch [12/300], Loss: 0.7510, Val Loss: 0.7555\n",
            "Epoch [13/300], Loss: 0.7490, Val Loss: 0.7534\n",
            "Epoch [14/300], Loss: 0.7470, Val Loss: 0.7513\n",
            "Epoch [15/300], Loss: 0.7451, Val Loss: 0.7493\n",
            "Epoch [16/300], Loss: 0.7433, Val Loss: 0.7474\n",
            "Epoch [17/300], Loss: 0.7415, Val Loss: 0.7455\n",
            "Epoch [18/300], Loss: 0.7398, Val Loss: 0.7436\n",
            "Epoch [19/300], Loss: 0.7381, Val Loss: 0.7418\n",
            "Epoch [20/300], Loss: 0.7364, Val Loss: 0.7400\n",
            "Epoch [21/300], Loss: 0.7348, Val Loss: 0.7382\n",
            "Epoch [22/300], Loss: 0.7332, Val Loss: 0.7365\n",
            "Epoch [23/300], Loss: 0.7316, Val Loss: 0.7349\n",
            "Epoch [24/300], Loss: 0.7301, Val Loss: 0.7332\n",
            "Epoch [25/300], Loss: 0.7286, Val Loss: 0.7316\n",
            "Epoch [26/300], Loss: 0.7271, Val Loss: 0.7300\n",
            "Epoch [27/300], Loss: 0.7257, Val Loss: 0.7285\n",
            "Epoch [28/300], Loss: 0.7243, Val Loss: 0.7270\n",
            "Epoch [29/300], Loss: 0.7229, Val Loss: 0.7255\n",
            "Epoch [30/300], Loss: 0.7215, Val Loss: 0.7240\n",
            "Epoch [31/300], Loss: 0.7202, Val Loss: 0.7226\n",
            "Epoch [32/300], Loss: 0.7189, Val Loss: 0.7212\n",
            "Epoch [33/300], Loss: 0.7176, Val Loss: 0.7198\n",
            "Epoch [34/300], Loss: 0.7164, Val Loss: 0.7185\n",
            "Epoch [35/300], Loss: 0.7152, Val Loss: 0.7171\n",
            "Epoch [36/300], Loss: 0.7139, Val Loss: 0.7158\n",
            "Epoch [37/300], Loss: 0.7128, Val Loss: 0.7145\n",
            "Epoch [38/300], Loss: 0.7116, Val Loss: 0.7133\n",
            "Epoch [39/300], Loss: 0.7105, Val Loss: 0.7120\n",
            "Epoch [40/300], Loss: 0.7094, Val Loss: 0.7108\n",
            "Epoch [41/300], Loss: 0.7083, Val Loss: 0.7096\n",
            "Epoch [42/300], Loss: 0.7072, Val Loss: 0.7085\n",
            "Epoch [43/300], Loss: 0.7061, Val Loss: 0.7073\n",
            "Epoch [44/300], Loss: 0.7051, Val Loss: 0.7062\n",
            "Epoch [45/300], Loss: 0.7041, Val Loss: 0.7051\n",
            "Epoch [46/300], Loss: 0.7031, Val Loss: 0.7040\n",
            "Epoch [47/300], Loss: 0.7021, Val Loss: 0.7030\n",
            "Epoch [48/300], Loss: 0.7012, Val Loss: 0.7019\n",
            "Epoch [49/300], Loss: 0.7003, Val Loss: 0.7009\n",
            "Epoch [50/300], Loss: 0.6993, Val Loss: 0.6999\n",
            "Epoch [51/300], Loss: 0.6984, Val Loss: 0.6989\n",
            "Epoch [52/300], Loss: 0.6976, Val Loss: 0.6979\n",
            "Epoch [53/300], Loss: 0.6967, Val Loss: 0.6970\n",
            "Epoch [54/300], Loss: 0.6959, Val Loss: 0.6960\n",
            "Epoch [55/300], Loss: 0.6950, Val Loss: 0.6951\n",
            "Epoch [56/300], Loss: 0.6942, Val Loss: 0.6942\n",
            "Epoch [57/300], Loss: 0.6934, Val Loss: 0.6934\n",
            "Epoch [58/300], Loss: 0.6927, Val Loss: 0.6925\n",
            "Epoch [59/300], Loss: 0.6919, Val Loss: 0.6917\n",
            "Epoch [60/300], Loss: 0.6912, Val Loss: 0.6908\n",
            "Epoch [61/300], Loss: 0.6904, Val Loss: 0.6900\n",
            "Epoch [62/300], Loss: 0.6897, Val Loss: 0.6892\n",
            "Epoch [63/300], Loss: 0.6890, Val Loss: 0.6884\n",
            "Epoch [64/300], Loss: 0.6883, Val Loss: 0.6877\n",
            "Epoch [65/300], Loss: 0.6877, Val Loss: 0.6869\n",
            "Epoch [66/300], Loss: 0.6870, Val Loss: 0.6862\n",
            "Epoch [67/300], Loss: 0.6864, Val Loss: 0.6855\n",
            "Epoch [68/300], Loss: 0.6857, Val Loss: 0.6848\n",
            "Epoch [69/300], Loss: 0.6851, Val Loss: 0.6841\n",
            "Epoch [70/300], Loss: 0.6845, Val Loss: 0.6834\n",
            "Epoch [71/300], Loss: 0.6839, Val Loss: 0.6827\n",
            "Epoch [72/300], Loss: 0.6834, Val Loss: 0.6821\n",
            "Epoch [73/300], Loss: 0.6828, Val Loss: 0.6815\n",
            "Epoch [74/300], Loss: 0.6822, Val Loss: 0.6808\n",
            "Epoch [75/300], Loss: 0.6817, Val Loss: 0.6802\n",
            "Epoch [76/300], Loss: 0.6812, Val Loss: 0.6796\n",
            "Epoch [77/300], Loss: 0.6807, Val Loss: 0.6790\n",
            "Epoch [78/300], Loss: 0.6802, Val Loss: 0.6785\n",
            "Epoch [79/300], Loss: 0.6797, Val Loss: 0.6779\n",
            "Epoch [80/300], Loss: 0.6792, Val Loss: 0.6774\n",
            "Epoch [81/300], Loss: 0.6787, Val Loss: 0.6768\n",
            "Epoch [82/300], Loss: 0.6783, Val Loss: 0.6763\n",
            "Epoch [83/300], Loss: 0.6778, Val Loss: 0.6758\n",
            "Epoch [84/300], Loss: 0.6774, Val Loss: 0.6753\n",
            "Epoch [85/300], Loss: 0.6770, Val Loss: 0.6748\n",
            "Epoch [86/300], Loss: 0.6765, Val Loss: 0.6743\n",
            "Epoch [87/300], Loss: 0.6761, Val Loss: 0.6738\n",
            "Epoch [88/300], Loss: 0.6757, Val Loss: 0.6734\n",
            "Epoch [89/300], Loss: 0.6753, Val Loss: 0.6729\n",
            "Epoch [90/300], Loss: 0.6750, Val Loss: 0.6725\n",
            "Epoch [91/300], Loss: 0.6746, Val Loss: 0.6720\n",
            "Epoch [92/300], Loss: 0.6742, Val Loss: 0.6716\n",
            "Epoch [93/300], Loss: 0.6739, Val Loss: 0.6712\n",
            "Epoch [94/300], Loss: 0.6735, Val Loss: 0.6708\n",
            "Epoch [95/300], Loss: 0.6732, Val Loss: 0.6704\n",
            "Epoch [96/300], Loss: 0.6729, Val Loss: 0.6700\n",
            "Epoch [97/300], Loss: 0.6726, Val Loss: 0.6696\n",
            "Epoch [98/300], Loss: 0.6722, Val Loss: 0.6693\n",
            "Epoch [99/300], Loss: 0.6719, Val Loss: 0.6689\n",
            "Epoch [100/300], Loss: 0.6716, Val Loss: 0.6685\n",
            "Epoch [101/300], Loss: 0.6714, Val Loss: 0.6682\n",
            "Epoch [102/300], Loss: 0.6711, Val Loss: 0.6679\n",
            "Epoch [103/300], Loss: 0.6708, Val Loss: 0.6675\n",
            "Epoch [104/300], Loss: 0.6705, Val Loss: 0.6672\n",
            "Epoch [105/300], Loss: 0.6703, Val Loss: 0.6669\n",
            "Epoch [106/300], Loss: 0.6700, Val Loss: 0.6666\n",
            "Epoch [107/300], Loss: 0.6698, Val Loss: 0.6663\n",
            "Epoch [108/300], Loss: 0.6695, Val Loss: 0.6660\n",
            "Epoch [109/300], Loss: 0.6693, Val Loss: 0.6657\n",
            "Epoch [110/300], Loss: 0.6691, Val Loss: 0.6654\n",
            "Epoch [111/300], Loss: 0.6688, Val Loss: 0.6651\n",
            "Epoch [112/300], Loss: 0.6686, Val Loss: 0.6649\n",
            "Epoch [113/300], Loss: 0.6684, Val Loss: 0.6646\n",
            "Epoch [114/300], Loss: 0.6682, Val Loss: 0.6644\n",
            "Epoch [115/300], Loss: 0.6680, Val Loss: 0.6641\n",
            "Epoch [116/300], Loss: 0.6678, Val Loss: 0.6639\n",
            "Epoch [117/300], Loss: 0.6676, Val Loss: 0.6636\n",
            "Epoch [118/300], Loss: 0.6674, Val Loss: 0.6634\n",
            "Epoch [119/300], Loss: 0.6673, Val Loss: 0.6632\n",
            "Epoch [120/300], Loss: 0.6671, Val Loss: 0.6630\n",
            "Epoch [121/300], Loss: 0.6669, Val Loss: 0.6627\n",
            "Epoch [122/300], Loss: 0.6668, Val Loss: 0.6625\n",
            "Epoch [123/300], Loss: 0.6666, Val Loss: 0.6623\n",
            "Epoch [124/300], Loss: 0.6664, Val Loss: 0.6621\n",
            "Epoch [125/300], Loss: 0.6663, Val Loss: 0.6619\n",
            "Epoch [126/300], Loss: 0.6661, Val Loss: 0.6618\n",
            "Epoch [127/300], Loss: 0.6660, Val Loss: 0.6616\n",
            "Epoch [128/300], Loss: 0.6659, Val Loss: 0.6614\n",
            "Epoch [129/300], Loss: 0.6657, Val Loss: 0.6612\n",
            "Epoch [130/300], Loss: 0.6656, Val Loss: 0.6610\n",
            "Epoch [131/300], Loss: 0.6655, Val Loss: 0.6609\n",
            "Epoch [132/300], Loss: 0.6653, Val Loss: 0.6607\n",
            "Epoch [133/300], Loss: 0.6652, Val Loss: 0.6605\n",
            "Epoch [134/300], Loss: 0.6651, Val Loss: 0.6604\n",
            "Epoch [135/300], Loss: 0.6650, Val Loss: 0.6602\n",
            "Epoch [136/300], Loss: 0.6649, Val Loss: 0.6601\n",
            "Epoch [137/300], Loss: 0.6648, Val Loss: 0.6599\n",
            "Epoch [138/300], Loss: 0.6647, Val Loss: 0.6598\n",
            "Epoch [139/300], Loss: 0.6646, Val Loss: 0.6597\n",
            "Epoch [140/300], Loss: 0.6645, Val Loss: 0.6595\n",
            "Epoch [141/300], Loss: 0.6644, Val Loss: 0.6594\n",
            "Epoch [142/300], Loss: 0.6643, Val Loss: 0.6593\n",
            "Epoch [143/300], Loss: 0.6642, Val Loss: 0.6592\n",
            "Epoch [144/300], Loss: 0.6641, Val Loss: 0.6590\n",
            "Epoch [145/300], Loss: 0.6640, Val Loss: 0.6589\n",
            "Epoch [146/300], Loss: 0.6639, Val Loss: 0.6588\n",
            "Epoch [147/300], Loss: 0.6639, Val Loss: 0.6587\n",
            "Epoch [148/300], Loss: 0.6638, Val Loss: 0.6586\n",
            "Epoch [149/300], Loss: 0.6637, Val Loss: 0.6585\n",
            "Epoch [150/300], Loss: 0.6636, Val Loss: 0.6584\n",
            "Epoch [151/300], Loss: 0.6636, Val Loss: 0.6583\n",
            "Epoch [152/300], Loss: 0.6635, Val Loss: 0.6582\n",
            "Epoch [153/300], Loss: 0.6634, Val Loss: 0.6581\n",
            "Epoch [154/300], Loss: 0.6634, Val Loss: 0.6580\n",
            "Epoch [155/300], Loss: 0.6633, Val Loss: 0.6579\n",
            "Epoch [156/300], Loss: 0.6633, Val Loss: 0.6578\n",
            "Epoch [157/300], Loss: 0.6632, Val Loss: 0.6577\n",
            "Epoch [158/300], Loss: 0.6631, Val Loss: 0.6576\n",
            "Epoch [159/300], Loss: 0.6631, Val Loss: 0.6576\n",
            "Epoch [160/300], Loss: 0.6630, Val Loss: 0.6575\n",
            "Epoch [161/300], Loss: 0.6630, Val Loss: 0.6574\n",
            "Epoch [162/300], Loss: 0.6629, Val Loss: 0.6573\n",
            "Epoch [163/300], Loss: 0.6629, Val Loss: 0.6573\n",
            "Epoch [164/300], Loss: 0.6628, Val Loss: 0.6572\n",
            "Epoch [165/300], Loss: 0.6628, Val Loss: 0.6571\n",
            "Epoch [166/300], Loss: 0.6628, Val Loss: 0.6571\n",
            "Epoch [167/300], Loss: 0.6627, Val Loss: 0.6570\n",
            "Epoch [168/300], Loss: 0.6627, Val Loss: 0.6569\n",
            "Epoch [169/300], Loss: 0.6626, Val Loss: 0.6569\n",
            "Epoch [170/300], Loss: 0.6626, Val Loss: 0.6568\n",
            "Epoch [171/300], Loss: 0.6626, Val Loss: 0.6567\n",
            "Epoch [172/300], Loss: 0.6625, Val Loss: 0.6567\n",
            "Epoch [173/300], Loss: 0.6625, Val Loss: 0.6566\n",
            "Epoch [174/300], Loss: 0.6625, Val Loss: 0.6566\n",
            "Epoch [175/300], Loss: 0.6624, Val Loss: 0.6565\n",
            "Epoch [176/300], Loss: 0.6624, Val Loss: 0.6565\n",
            "Epoch [177/300], Loss: 0.6624, Val Loss: 0.6564\n",
            "Epoch [178/300], Loss: 0.6623, Val Loss: 0.6564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle training/testing split based on epochs\n",
        "# make sure the padding works out properly\n",
        "# try because otherwise... more input"
      ],
      "metadata": {
        "id": "jbgBj4KyeE3O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}